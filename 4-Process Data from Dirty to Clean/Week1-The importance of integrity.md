### Data integrity
The accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.

Data integrity can be compromised in lots of different ways. There's a chance data can be compromised every time it's replicated, transferred, or manipulated in any way.
- **Data replication**: The process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your data will be out of sync. This data lacks integrity because different people might not be using the same data for their findings, which can cause inconsistencies.
- **Data transfer**: The process of copying data from a storage device to memory, or from one computer to another. If your data transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs.
- **Data manipulation**: The process of changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process more efficient, but an error during the process can compromise the efficiency.
- **Other threats**: Data can also be compromised through human error, viruses, malware, hacking, and system failures.

Clean data + alignment to business objective = accurate conclusions <br>
Alignment to business objective + newly discovered variables + constraints = accurate conclusions 

---

Types of insufficient data:
- Data from only one source
- Data that keeps updating
- Outdated data
- Geographically-limited data

Ways to address insufficient data:
- Identify trends with the available data
- Wait for more data if time allows
- Talk with stakeholders and adjust your objective
- Look for a new dataset

<details><summary>Consider the following data issues and suggestions on how to work around them.</summary>
  <h5>Data issue 1: no data</h5><ul><li>Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data. </li><li>If there isn’t time to collect data, perform the analysis using proxy data from other datasets. This is the most common workaround.</li></ul>
  
  <h5>Data issue 2: too little data</h5><ul><li>Do the analysis using proxy data along with actual data.</li><li>Adjust your analysis to align with the data you already have.</li></ul>

  <h5>Data issue 3: wrong data, including data with errors</h5><ul><li>If you have the wrong data because requirements were misunderstood, communicate the requirements again.</li><li>Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors.</li><li>If you can’t correct data errors yourself, you can ignore the wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias. </li></ul>
  
![image](https://user-images.githubusercontent.com/74421758/147212712-c7f0263e-c40a-4cdb-b290-8adc2ec3499a.png)
  
</details>

#### Random sampling
A way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.

[Caluculating sample size](https://drive.google.com/file/d/1D_yQ1ph_I4F7D-5nVwx5iJ9YmbcO7-cC/view?usp=sharing)

Pre-cleaning activities help you determine and maintain data integrity and are important because they increase the efficiency and success of your data analysis tasks. One of the objectives of pre-cleaning activities is to address insufficient data. <br> If you know that your data is accurate, consistent, and complete, you can be confident that your results will be valid. Stakeholders will be pleased if you connect the data to business objectives. And, knowing when to stop collecting data will allow you to finish your tasks in a timely manner without sacrificing data integrity. Data analysts perform pre-cleaning activities to complete these steps.

---

#### Statistical power
The probability of getting meaningful results from a test. Statistical power can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment.

#### Hypothesis testing
A way to see if a survey or experiment has meaningful results. 

Statistical power is usually shown as a value out of one. If a test is statistically significant, it means the results of the test are real and not an error caused by random chance. Usually, you need a statistical power of at least 0.8 or 80% to consider your results statistically significant.

#### Confidence level
The probability that your sample accurately reflects the greater population. Having a 99 percent confidence level is ideal but most industries hope for at least a 90 or 95 percent confidence level.

A sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. To calculate sample size using an online calculator, it’s necessary to input the confidence level, margin of error, and population size. [Calculator](https://docs.google.com/spreadsheets/d/1kBTvnpH2qOLJx4XWjUG1v-GF4LPmOhequy_9VRyslJ8/template/preview). The calculated sample size is the minimum number to achieve what you input for confidence level and margin of error. If you are working with a survey, you will also need to think about the estimated response rate to figure out how many surveys you will need to send out.

---
