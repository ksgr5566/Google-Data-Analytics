### Data integrity
The accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.

Data integrity can be compromised in lots of different ways. There's a chance data can be compromised every time it's replicated, transferred, or manipulated in any way.
- **Data replication**: The process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your data will be out of sync. This data lacks integrity because different people might not be using the same data for their findings, which can cause inconsistencies.
- **Data transfer**: The process of copying data from a storage device to memory, or from one computer to another. If your data transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs.
- **Data manipulation**: The process of changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process more efficient, but an error during the process can compromise the efficiency.
- **Other threats**: Data can also be compromised through human error, viruses, malware, hacking, and system failures.

Clean data + alignment to business objective = accurate conclusions <br>
Alignment to business objective + newly discovered variables + constraints = accurate conclusions 

---

Types of insufficient data:
- Data from only one source
- Data that keeps updating
- Outdated data
- Geographically-limited data

Ways to address insufficient data:
- Identify trends with the available data
- Wait for more data if time allows
- Talk with stakeholders and adjust your objective
- Look for a new dataset

<details><summary>Consider the following data issues and suggestions on how to work around them.</summary>
  <h5>Data issue 1: no data</h5><ul><li>Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data. </li><li>If there isn’t time to collect data, perform the analysis using proxy data from other datasets. This is the most common workaround.</li></ul>
  
  <h5>Data issue 2: too little data</h5><ul><li>Do the analysis using proxy data along with actual data.</li><li>Adjust your analysis to align with the data you already have.</li></ul>

  <h5>Data issue 3: wrong data, including data with errors</h5><ul><li>If you have the wrong data because requirements were misunderstood, communicate the requirements again.</li><li>Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors.</li><li>If you can’t correct data errors yourself, you can ignore the wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias. </li></ul>
  
![image](https://user-images.githubusercontent.com/74421758/147212712-c7f0263e-c40a-4cdb-b290-8adc2ec3499a.png)
  
</details>

#### Random sampling
A way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.

[Caluculating sample size](https://drive.google.com/file/d/1D_yQ1ph_I4F7D-5nVwx5iJ9YmbcO7-cC/view?usp=sharing)

Pre-cleaning activities help you determine and maintain data integrity and are important because they increase the efficiency and success of your data analysis tasks. One of the objectives of pre-cleaning activities is to address insufficient data. <br> If you know that your data is accurate, consistent, and complete, you can be confident that your results will be valid. Stakeholders will be pleased if you connect the data to business objectives. And, knowing when to stop collecting data will allow you to finish your tasks in a timely manner without sacrificing data integrity. Data analysts perform pre-cleaning activities to complete these steps.

---

